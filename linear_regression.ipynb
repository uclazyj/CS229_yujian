{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77185db0-cb7b-4229-b18d-adb52bec5880",
   "metadata": {},
   "source": [
    "# Linear regression and gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8329f0-0fbb-4ae2-922c-42780f673998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "k = 3\n",
    "b = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18b057-72bc-49ad-909b-fa367a78c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 20 # Number of data points\n",
    "x = 1 * np.random.rand(m)\n",
    "\n",
    "A = 0.1 # Amplitude of noise\n",
    "noise = A * np.random.rand(m) - A/2\n",
    "y = k * x + b + noise\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c34a092-e847-4d2b-8d7e-a8cca86691ff",
   "metadata": {},
   "source": [
    "## Use gradient descent to find the linear fit\n",
    "\n",
    "Given an input vector ${x}^{(i)}$ with $n$ features, the hypothesis:\n",
    "$$h(x^{(i)}) = \\sum_{j=0}^n \\theta_j x_j^{(i)} $$\n",
    "where $x_0^{(i)}$ is defined to be 1. The cost function is:\n",
    "$$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^m (h(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "The $j_{th}$ component of the gradient:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\sum_{i=1}^m (h(x^{(i)}) - y^{(i)}) x_j^{(i)}$$\n",
    "\n",
    "So the gradient descent update rule is:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\theta_j - \\alpha \\sum_{i=1}^m (h(x^{(i)}) - y^{(i)}) x_j^{(i)}$$\n",
    "\n",
    "When actually implementing it, it is helpful to write in a matrix form:\n",
    "\n",
    "$$ \\theta := \\theta - \\alpha \\nabla_\\theta J(\\theta)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = X^T(X\\Theta - Y)$$\n",
    "\n",
    "where $X$ is a $m$ (number of training examples) by $n+1$ (number of features) matrix, $\\Theta$ and $Y$ are both $n$ by 1 vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106853c2-03c4-4189-812b-328112c01e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones(m).reshape(m,1),x.reshape(m,1)))\n",
    "alpha = 0.01\n",
    "nsteps = 100\n",
    "theta = np.array([0,0])\n",
    "\n",
    "plot_steps = [1,3,10,25,100]\n",
    "\n",
    "nsteps = 100\n",
    "steps = list(range(1,1+nsteps))\n",
    "J = []\n",
    "\n",
    "for step in steps:\n",
    "    h = X @ theta\n",
    "    J.append((h - y) @ (h - y) / 2)\n",
    "    gradient = X.transpose() @ (h - y)\n",
    "    theta = theta - alpha * gradient\n",
    "    # potentially plot \n",
    "    if step in plot_steps:\n",
    "        plt.scatter(x,y)\n",
    "        y_predict = X @ theta\n",
    "        plt.plot(x, y_predict,'r')\n",
    "        plt.title('step = ' + str(step))\n",
    "        plt.show()\n",
    "\n",
    "plt.plot(steps, J)\n",
    "plt.xlabel('number of steps')\n",
    "plt.ylabel('J')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae81ae-560a-4122-be81-87125a59c916",
   "metadata": {},
   "source": [
    "By setting the gradient to 0, we get the normal equation:\n",
    "\n",
    "$$\\Theta = (X^TX)^{-1} X^TY $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c840dac-2f09-4fc3-90a6-de75c2c40e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "XT = X.transpose()\n",
    "theta_normal = np.linalg.inv(XT @ X) @ XT @ y\n",
    "\n",
    "plt.scatter(x,y)\n",
    "y_predict = X @ theta_normal\n",
    "plt.plot(x, y_predict,'r')\n",
    "plt.title('step = ' + str(step))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a0b1a-5555-41a4-9f48-94075bba4046",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent\n",
    "\n",
    "The Stochastic gradient descent update rule is:\n",
    "\n",
    "Repeatly for $i=1,2,...,m$, do\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J^{(i)}(\\theta) = \\theta_j - \\alpha (h(x^{(i)}) - y^{(i)}) x_j^{(i)}$$\n",
    "\n",
    "for every $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bfc87b-642a-409c-b585-57ad3cd3aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([0,0])\n",
    "J_stochastic = []\n",
    "\n",
    "for step in steps:\n",
    "    h = X @ theta\n",
    "    J_stochastic.append((h - y) @ (h - y) / 2)\n",
    "    for i in range(m):\n",
    "        h_i = X[i] @ theta\n",
    "        gradient = (h_i - y[i]) * X[i]\n",
    "        theta = theta - alpha * gradient\n",
    "        # potentially plot \n",
    "    if step in plot_steps:\n",
    "        plt.scatter(x,y)\n",
    "        y_predict = X @ theta\n",
    "        plt.plot(x, y_predict,'r')\n",
    "        plt.title('step = ' + str(step))\n",
    "        plt.show()\n",
    "\n",
    "plt.plot(steps, J)\n",
    "plt.plot(steps, J_stochastic)\n",
    "plt.xlabel('number of steps')\n",
    "plt.ylabel('J')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db381471-1656-4478-91c9-e893fc3086db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
